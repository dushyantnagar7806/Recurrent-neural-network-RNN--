{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to Prepare Text Data for Deep Learning with Keras\n\nYou cannot feed raw text directly into deep learning models.\n\nText data must be encoded as numbers to be used as input or output for machine learning and deep learning models.\n\nThe Keras deep learning library provides some basic tools to help you prepare your text data.\n\nIn this tutorial, you will discover how you can use Keras to prepare your text data.\n\n# 1.  Integer Encoding in RNN\n\nSo far we have looked at one-off convenience methods for preparing text with Keras.\n\nKeras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects.\n\nKeras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents.\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndocs = ['go india',\n\t\t'india india',\n\t\t'hip hip hurray',\n\t\t'jeetega bhai jeetega india jeetega',\n\t\t'bharat mata ki jai',\n\t\t'kohli kohli',\n\t\t'sachin sachin',\n\t\t'dhoni dhoni',\n\t\t'modi ji ki jai',\n\t\t'inquilab zindabad']","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:44:57.089179Z","iopub.execute_input":"2023-08-04T13:44:57.089629Z","iopub.status.idle":"2023-08-04T13:44:57.131288Z","shell.execute_reply.started":"2023-08-04T13:44:57.089595Z","shell.execute_reply":"2023-08-04T13:44:57.129974Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# import Tokenizer\nfrom keras.preprocessing.text import Tokenizer\n\n# create the tokenizer\ntokenizer = Tokenizer(oov_token='<nothing>')    # oov_token -> out of vocabulary (its mean the if the input come this netwrok and word not come in input then write nothing)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:44:59.750119Z","iopub.execute_input":"2023-08-04T13:44:59.750710Z","iopub.status.idle":"2023-08-04T13:45:08.402902Z","shell.execute_reply.started":"2023-08-04T13:44:59.750660Z","shell.execute_reply":"2023-08-04T13:45:08.401653Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"#  fit the tokenizer on the documents\ntokenizer.fit_on_texts(docs)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:45:14.496815Z","iopub.execute_input":"2023-08-04T13:45:14.497564Z","iopub.status.idle":"2023-08-04T13:45:14.503177Z","shell.execute_reply.started":"2023-08-04T13:45:14.497529Z","shell.execute_reply":"2023-08-04T13:45:14.502013Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"- Once fit, the Tokenizer provides **4 attributes** that you can use to query what has been learned about your documents:\n\n     - **word_counts:** A dictionary of words and their counts.\n     - **word_docs:** A dictionary of words and how many documents each appeared in.\n     - **word_index:** A dictionary of words and their uniquely assigned integers.\n     - **document_count:** An integer count of the total number of documents that were used to fit the Tokenizer.","metadata":{}},{"cell_type":"code","source":"# index of vocabulary\ntokenizer.word_index ","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:45:22.677083Z","iopub.execute_input":"2023-08-04T13:45:22.677495Z","iopub.status.idle":"2023-08-04T13:45:22.687255Z","shell.execute_reply.started":"2023-08-04T13:45:22.677463Z","shell.execute_reply":"2023-08-04T13:45:22.685098Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'<nothing>': 1,\n 'india': 2,\n 'jeetega': 3,\n 'hip': 4,\n 'ki': 5,\n 'jai': 6,\n 'kohli': 7,\n 'sachin': 8,\n 'dhoni': 9,\n 'go': 10,\n 'hurray': 11,\n 'bhai': 12,\n 'bharat': 13,\n 'mata': 14,\n 'modi': 15,\n 'ji': 16,\n 'inquilab': 17,\n 'zindabad': 18}"},"metadata":{}}]},{"cell_type":"code","source":"# totle counts word how mant times comes in corpus\ntokenizer.word_counts","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:45:26.648956Z","iopub.execute_input":"2023-08-04T13:45:26.649448Z","iopub.status.idle":"2023-08-04T13:45:26.660282Z","shell.execute_reply.started":"2023-08-04T13:45:26.649408Z","shell.execute_reply":"2023-08-04T13:45:26.658326Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('go', 1),\n             ('india', 4),\n             ('hip', 2),\n             ('hurray', 1),\n             ('jeetega', 3),\n             ('bhai', 1),\n             ('bharat', 1),\n             ('mata', 1),\n             ('ki', 2),\n             ('jai', 2),\n             ('kohli', 2),\n             ('sachin', 2),\n             ('dhoni', 2),\n             ('modi', 1),\n             ('ji', 1),\n             ('inquilab', 1),\n             ('zindabad', 1)])"},"metadata":{}}]},{"cell_type":"code","source":"# totle docoments present in data\ntokenizer.document_count","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:45:43.006744Z","iopub.execute_input":"2023-08-04T13:45:43.007476Z","iopub.status.idle":"2023-08-04T13:45:43.014323Z","shell.execute_reply.started":"2023-08-04T13:45:43.007440Z","shell.execute_reply":"2023-08-04T13:45:43.013144Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"code","source":"#  A dictionary of words and how many documents each appeared in.\ntokenizer.word_docs","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:45:46.326184Z","iopub.execute_input":"2023-08-04T13:45:46.326564Z","iopub.status.idle":"2023-08-04T13:45:46.334214Z","shell.execute_reply.started":"2023-08-04T13:45:46.326533Z","shell.execute_reply":"2023-08-04T13:45:46.332971Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"defaultdict(int,\n            {'go': 1,\n             'india': 3,\n             'hurray': 1,\n             'hip': 1,\n             'jeetega': 1,\n             'bhai': 1,\n             'mata': 1,\n             'jai': 2,\n             'bharat': 1,\n             'ki': 2,\n             'kohli': 1,\n             'sachin': 1,\n             'dhoni': 1,\n             'ji': 1,\n             'modi': 1,\n             'zindabad': 1,\n             'inquilab': 1})"},"metadata":{}}]},{"cell_type":"code","source":"# convert the text to numberic based on the vocabulary index\n# use tokenizer.text_to_sequences\nsequences = tokenizer.texts_to_sequences(docs)\nsequences","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:50.225489Z","iopub.execute_input":"2023-07-02T18:44:50.226777Z","iopub.status.idle":"2023-07-02T18:44:50.237942Z","shell.execute_reply.started":"2023-07-02T18:44:50.226729Z","shell.execute_reply":"2023-07-02T18:44:50.236857Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[[10, 2],\n [2, 2],\n [4, 4, 11],\n [3, 12, 3, 2, 3],\n [13, 14, 5, 6],\n [7, 7],\n [8, 8],\n [9, 9],\n [15, 16, 5, 6],\n [17, 18]]"},"metadata":{}}]},{"cell_type":"code","source":"# this all docoments have different different dimesnion \n# convert the all docoments same direction \n# use pad_sequences\nfrom keras.utils import pad_sequences\nsequences = pad_sequences(sequences,padding='post')    # padding post mena the zero is present the number of vocaulary 'pre ,post'","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:50.240609Z","iopub.execute_input":"2023-07-02T18:44:50.241419Z","iopub.status.idle":"2023-07-02T18:44:50.250551Z","shell.execute_reply.started":"2023-07-02T18:44:50.241374Z","shell.execute_reply":"2023-07-02T18:44:50.249059Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sequences","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:50.252504Z","iopub.execute_input":"2023-07-02T18:44:50.252979Z","iopub.status.idle":"2023-07-02T18:44:50.267267Z","shell.execute_reply.started":"2023-07-02T18:44:50.252941Z","shell.execute_reply":"2023-07-02T18:44:50.266159Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([[10,  2,  0,  0,  0],\n       [ 2,  2,  0,  0,  0],\n       [ 4,  4, 11,  0,  0],\n       [ 3, 12,  3,  2,  3],\n       [13, 14,  5,  6,  0],\n       [ 7,  7,  0,  0,  0],\n       [ 8,  8,  0,  0,  0],\n       [ 9,  9,  0,  0,  0],\n       [15, 16,  5,  6,  0],\n       [17, 18,  0,  0,  0]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"from keras.datasets import imdb\nfrom keras import Sequential\nfrom keras.layers import Dense,SimpleRNN,Flatten","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:58:43.624197Z","iopub.execute_input":"2023-08-04T13:58:43.624643Z","iopub.status.idle":"2023-08-04T13:58:43.634032Z","shell.execute_reply.started":"2023-08-04T13:58:43.624611Z","shell.execute_reply":"2023-08-04T13:58:43.632688Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# load imbd data\n(X_train,y_train),(X_test,y_test) = imdb.load_data()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:58:46.083267Z","iopub.execute_input":"2023-08-04T13:58:46.083652Z","iopub.status.idle":"2023-08-04T13:58:52.424317Z","shell.execute_reply.started":"2023-08-04T13:58:46.083623Z","shell.execute_reply":"2023-08-04T13:58:52.423209Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17464789/17464789 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:58:52.426732Z","iopub.execute_input":"2023-08-04T13:58:52.427063Z","iopub.status.idle":"2023-08-04T13:58:52.433056Z","shell.execute_reply.started":"2023-08-04T13:58:52.427037Z","shell.execute_reply":"2023-08-04T13:58:52.431859Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(25000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2023-08-04T13:58:57.308477Z","iopub.execute_input":"2023-08-04T13:58:57.309034Z","iopub.status.idle":"2023-08-04T13:58:57.317926Z","shell.execute_reply.started":"2023-08-04T13:58:57.308994Z","shell.execute_reply":"2023-08-04T13:58:57.316791Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n       ...,\n       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"**This data is already preprocesed and integier encoded**","metadata":{}},{"cell_type":"code","source":"print(len(X_train[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:56.859972Z","iopub.execute_input":"2023-07-02T18:44:56.860826Z","iopub.status.idle":"2023-07-02T18:44:56.872843Z","shell.execute_reply.started":"2023-07-02T18:44:56.860779Z","shell.execute_reply":"2023-07-02T18:44:56.871260Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"218\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(X_train[1]))  # all the have different different dimesion ","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:56.874288Z","iopub.execute_input":"2023-07-02T18:44:56.874824Z","iopub.status.idle":"2023-07-02T18:44:56.888742Z","shell.execute_reply.started":"2023-07-02T18:44:56.874787Z","shell.execute_reply":"2023-07-02T18:44:56.887740Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"189\n","output_type":"stream"}]},{"cell_type":"code","source":"# convert the same dimension \n# add pad_sequences\nX_train = pad_sequences(X_train,padding='post',maxlen=50)\nX_test = pad_sequences(X_test,padding='post',maxlen=50)  # maxlen : means the dimension of 50 ","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:56.891663Z","iopub.execute_input":"2023-07-02T18:44:56.892680Z","iopub.status.idle":"2023-07-02T18:44:57.528111Z","shell.execute_reply.started":"2023-07-02T18:44:56.892622Z","shell.execute_reply":"2023-07-02T18:44:57.526596Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)  # you can see the simaply before and after the differnce ","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:57.529804Z","iopub.execute_input":"2023-07-02T18:44:57.530552Z","iopub.status.idle":"2023-07-02T18:44:57.536574Z","shell.execute_reply.started":"2023-07-02T18:44:57.530516Z","shell.execute_reply":"2023-07-02T18:44:57.535149Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"(25000, 50)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_train[0].shape)\nprint(X_train[1].shape)\nprint(X_train[2].shape)  # all of this same dimesntion \n","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:57.538808Z","iopub.execute_input":"2023-07-02T18:44:57.539181Z","iopub.status.idle":"2023-07-02T18:44:57.552329Z","shell.execute_reply.started":"2023-07-02T18:44:57.539150Z","shell.execute_reply":"2023-07-02T18:44:57.551085Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"(50,)\n(50,)\n(50,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**input_shape=(time_step,input_feature)**","metadata":{}},{"cell_type":"code","source":"# model \nmodel = Sequential()\n\nmodel.add(SimpleRNN(32,input_shape=(50,1),return_sequences=False))   # 32 is node , input shape is (timestamp,feature)\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:57.553604Z","iopub.execute_input":"2023-07-02T18:44:57.554056Z","iopub.status.idle":"2023-07-02T18:44:57.758318Z","shell.execute_reply.started":"2023-07-02T18:44:57.554025Z","shell.execute_reply":"2023-07-02T18:44:57.756880Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 32)                1088      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1,121\nTrainable params: 1,121\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:44:57.760414Z","iopub.execute_input":"2023-07-02T18:44:57.760841Z","iopub.status.idle":"2023-07-02T18:46:21.618252Z","shell.execute_reply.started":"2023-07-02T18:44:57.760808Z","shell.execute_reply":"2023-07-02T18:46:21.616594Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Epoch 1/5\n782/782 [==============================] - 15s 17ms/step - loss: 0.6944 - accuracy: 0.5013 - val_loss: 0.6951 - val_accuracy: 0.5031\nEpoch 2/5\n782/782 [==============================] - 14s 18ms/step - loss: 0.6933 - accuracy: 0.5078 - val_loss: 0.6936 - val_accuracy: 0.5077\nEpoch 3/5\n782/782 [==============================] - 14s 17ms/step - loss: 0.6932 - accuracy: 0.5041 - val_loss: 0.6938 - val_accuracy: 0.5058\nEpoch 4/5\n782/782 [==============================] - 12s 15ms/step - loss: 0.6930 - accuracy: 0.5047 - val_loss: 0.6969 - val_accuracy: 0.5022\nEpoch 5/5\n782/782 [==============================] - 14s 18ms/step - loss: 0.6928 - accuracy: 0.5075 - val_loss: 0.6945 - val_accuracy: 0.5026\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f88a51f3310>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2. Embading Encoding \n\nA word embedding is a learned representation for text where words that have the same meaning and save similar representation\n\n- This approach to representing words and documents may be considered one of the key breakthroughs of deep learning on challenging NLP problems\n\n- Word embeddings are alternative to one-hot encoding along with dimensionality reduction.\n\n\n**`One-hot word vectors`** — Sparse, High-dimensional and Hard-coded , not sementic mean\n\n**`Word embeddings`** — Dense, Lower-Dimensional and Learned from the data\n\n\n- `Keras library has embeddings laye`r which does word representation of given text corpus\n\n     **tf.keras.layers.Embedding( input_dim, output_dim, embeddings_initializer=’uniform’, embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs)**\n\n`Key Arguments:`\n\n`input_dim` — the size of vocabulary or length of the word index\n\n`output_dim` — Output dimension of word representation\n\n`input_length` — max input sequence length of the document\n\n\nThe Emabdaing requred the numeric data\n","metadata":{}},{"cell_type":"code","source":"docs = ['go india',\n\t\t'india india',\n\t\t'hip hip hurray',\n\t\t'jeetega bhai jeetega india jeetega',\n\t\t'bharat mata ki jai',\n\t\t'kohli kohli',\n\t\t'sachin sachin',\n\t\t'dhoni dhoni',\n\t\t'modi ji ki jai',\n\t\t'inquilab zindabad']","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.620013Z","iopub.execute_input":"2023-07-02T18:46:21.621141Z","iopub.status.idle":"2023-07-02T18:46:21.628499Z","shell.execute_reply.started":"2023-07-02T18:46:21.621101Z","shell.execute_reply":"2023-07-02T18:46:21.626733Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.630619Z","iopub.execute_input":"2023-07-02T18:46:21.631148Z","iopub.status.idle":"2023-07-02T18:46:21.647875Z","shell.execute_reply.started":"2023-07-02T18:46:21.631096Z","shell.execute_reply":"2023-07-02T18:46:21.646518Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(docs)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.649193Z","iopub.execute_input":"2023-07-02T18:46:21.649581Z","iopub.status.idle":"2023-07-02T18:46:21.663704Z","shell.execute_reply.started":"2023-07-02T18:46:21.649550Z","shell.execute_reply":"2023-07-02T18:46:21.662246Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"len(tokenizer.word_index)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.664783Z","iopub.execute_input":"2023-07-02T18:46:21.665131Z","iopub.status.idle":"2023-07-02T18:46:21.685540Z","shell.execute_reply.started":"2023-07-02T18:46:21.665104Z","shell.execute_reply":"2023-07-02T18:46:21.683933Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"17"},"metadata":{}}]},{"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(docs)\nsequences","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.691855Z","iopub.execute_input":"2023-07-02T18:46:21.692275Z","iopub.status.idle":"2023-07-02T18:46:21.704535Z","shell.execute_reply.started":"2023-07-02T18:46:21.692243Z","shell.execute_reply":"2023-07-02T18:46:21.703175Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"[[9, 1],\n [1, 1],\n [3, 3, 10],\n [2, 11, 2, 1, 2],\n [12, 13, 4, 5],\n [6, 6],\n [7, 7],\n [8, 8],\n [14, 15, 4, 5],\n [16, 17]]"},"metadata":{}}]},{"cell_type":"code","source":"from keras.utils import pad_sequences\nsequences = pad_sequences(sequences,padding='post')\nsequences","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.705850Z","iopub.execute_input":"2023-07-02T18:46:21.706818Z","iopub.status.idle":"2023-07-02T18:46:21.722401Z","shell.execute_reply.started":"2023-07-02T18:46:21.706783Z","shell.execute_reply":"2023-07-02T18:46:21.721212Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"array([[ 9,  1,  0,  0,  0],\n       [ 1,  1,  0,  0,  0],\n       [ 3,  3, 10,  0,  0],\n       [ 2, 11,  2,  1,  2],\n       [12, 13,  4,  5,  0],\n       [ 6,  6,  0,  0,  0],\n       [ 7,  7,  0,  0,  0],\n       [ 8,  8,  0,  0,  0],\n       [14, 15,  4,  5,  0],\n       [16, 17,  0,  0,  0]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"from keras.layers import Dense,SimpleRNN,Embedding,Flatten","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.724176Z","iopub.execute_input":"2023-07-02T18:46:21.724514Z","iopub.status.idle":"2023-07-02T18:46:21.736196Z","shell.execute_reply.started":"2023-07-02T18:46:21.724486Z","shell.execute_reply":"2023-07-02T18:46:21.734763Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(18,output_dim=2,input_length=5))         # 18 is vocabulary , output_dim means the one word represent 2 vector form , input_lengrh mean\n                                                             # the docoment size is 5 its mean the sentance size is 5                   \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.737991Z","iopub.execute_input":"2023-07-02T18:46:21.738355Z","iopub.status.idle":"2023-07-02T18:46:21.786329Z","shell.execute_reply.started":"2023-07-02T18:46:21.738318Z","shell.execute_reply":"2023-07-02T18:46:21.785054Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 5, 2)              36        \n                                                                 \n=================================================================\nTotal params: 36\nTrainable params: 36\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile('adam','accuracy')","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.788128Z","iopub.execute_input":"2023-07-02T18:46:21.788501Z","iopub.status.idle":"2023-07-02T18:46:21.803471Z","shell.execute_reply.started":"2023-07-02T18:46:21.788470Z","shell.execute_reply":"2023-07-02T18:46:21.802087Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(sequences)\nprint(pred)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T18:46:21.805755Z","iopub.execute_input":"2023-07-02T18:46:21.806171Z","iopub.status.idle":"2023-07-02T18:46:21.984690Z","shell.execute_reply.started":"2023-07-02T18:46:21.806138Z","shell.execute_reply":"2023-07-02T18:46:21.982210Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 105ms/step\n[[[-0.03529511 -0.04260511]\n  [-0.03166655  0.02645114]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]]\n\n [[-0.03166655  0.02645114]\n  [-0.03166655  0.02645114]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]]\n\n [[ 0.0192664  -0.0036878 ]\n  [ 0.0192664  -0.0036878 ]\n  [ 0.04702375 -0.01738302]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]]\n\n [[-0.02891405 -0.00236194]\n  [-0.02816381 -0.01025749]\n  [-0.02891405 -0.00236194]\n  [-0.03166655  0.02645114]\n  [-0.02891405 -0.00236194]]\n\n [[-0.00816128 -0.04623958]\n  [ 0.0192096  -0.02086746]\n  [ 0.04635206 -0.024409  ]\n  [ 0.04513386  0.0170216 ]\n  [-0.00296923  0.03606423]]\n\n [[ 0.0280334  -0.02030165]\n  [ 0.0280334  -0.02030165]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]]\n\n [[ 0.03160164  0.02577854]\n  [ 0.03160164  0.02577854]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]]\n\n [[-0.00747525  0.00968815]\n  [-0.00747525  0.00968815]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]]\n\n [[-0.03929137 -0.02096378]\n  [-0.04674075 -0.0356249 ]\n  [ 0.04635206 -0.024409  ]\n  [ 0.04513386  0.0170216 ]\n  [-0.00296923  0.03606423]]\n\n [[ 0.02244424  0.01132399]\n  [ 0.02976594 -0.02648497]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]\n  [-0.00296923  0.03606423]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.datasets import imdb\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nfrom keras import Sequential\nfrom keras.layers import Dense,SimpleRNN,Embedding,Flatten","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:23:13.554128Z","iopub.execute_input":"2023-08-04T14:23:13.554584Z","iopub.status.idle":"2023-08-04T14:23:13.560902Z","shell.execute_reply.started":"2023-08-04T14:23:13.554552Z","shell.execute_reply":"2023-08-04T14:23:13.559755Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"(X_train,y_train),(X_test,y_test) = imdb.load_data()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:23:14.721832Z","iopub.execute_input":"2023-08-04T14:23:14.722900Z","iopub.status.idle":"2023-08-04T14:23:19.927711Z","shell.execute_reply.started":"2023-08-04T14:23:14.722859Z","shell.execute_reply":"2023-08-04T14:23:19.926604Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X_train = pad_sequences(X_train,padding='post',maxlen=50)\nX_test = pad_sequences(X_test,padding='post',maxlen=50)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:23:21.833094Z","iopub.execute_input":"2023-08-04T14:23:21.833508Z","iopub.status.idle":"2023-08-04T14:23:22.254320Z","shell.execute_reply.started":"2023-08-04T14:23:21.833477Z","shell.execute_reply":"2023-08-04T14:23:22.253338Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:23:25.233287Z","iopub.execute_input":"2023-08-04T14:23:25.233674Z","iopub.status.idle":"2023-08-04T14:23:25.242294Z","shell.execute_reply.started":"2023-08-04T14:23:25.233645Z","shell.execute_reply":"2023-08-04T14:23:25.240721Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(25000, 50)"},"metadata":{}}]},{"cell_type":"code","source":"X_train.max()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:28:16.148296Z","iopub.execute_input":"2023-08-04T14:28:16.148705Z","iopub.status.idle":"2023-08-04T14:28:16.155733Z","shell.execute_reply.started":"2023-08-04T14:28:16.148675Z","shell.execute_reply":"2023-08-04T14:28:16.154532Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"88585"},"metadata":{}}]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(88586, output_dim=2,input_length=50))\nmodel.add(SimpleRNN(32,return_sequences=False))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:28:47.278180Z","iopub.execute_input":"2023-08-04T14:28:47.279321Z","iopub.status.idle":"2023-08-04T14:28:47.410196Z","shell.execute_reply.started":"2023-08-04T14:28:47.279271Z","shell.execute_reply":"2023-08-04T14:28:47.408841Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Model: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_4 (Embedding)     (None, 50, 2)             177172    \n                                                                 \n simple_rnn_4 (SimpleRNN)    (None, 32)                1120      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 178,325\nTrainable params: 178,325\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nmodel.fit(X_train, y_train,epochs=5,validation_data=(X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:28:51.513014Z","iopub.execute_input":"2023-08-04T14:28:51.513498Z","iopub.status.idle":"2023-08-04T14:29:58.715886Z","shell.execute_reply.started":"2023-08-04T14:28:51.513461Z","shell.execute_reply":"2023-08-04T14:29:58.714770Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch 1/5\n782/782 [==============================] - 14s 16ms/step - loss: 0.5847 - acc: 0.6594 - val_loss: 0.4560 - val_acc: 0.7936\nEpoch 2/5\n782/782 [==============================] - 13s 17ms/step - loss: 0.3545 - acc: 0.8504 - val_loss: 0.4143 - val_acc: 0.8141\nEpoch 3/5\n782/782 [==============================] - 15s 19ms/step - loss: 0.2458 - acc: 0.9043 - val_loss: 0.4466 - val_acc: 0.8064\nEpoch 4/5\n782/782 [==============================] - 13s 16ms/step - loss: 0.1821 - acc: 0.9344 - val_loss: 0.4819 - val_acc: 0.8022\nEpoch 5/5\n782/782 [==============================] - 13s 16ms/step - loss: 0.1367 - acc: 0.9537 - val_loss: 0.5626 - val_acc: 0.7783\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x79b0a0301630>"},"metadata":{}}]}]}